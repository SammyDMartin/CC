{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SammyDMartin/CC/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "senodCwtRgN4",
        "outputId": "01815f6c-18f2-409b-c12e-3bdf6064a8c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n#Uncomment for Colab\\n!pip install \"ray[rllib]\" torch\\n!pip install nashpy\\n!pip install pickle5\\n!git clone https://github.com/SammyDMartin/CC\\n'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "#Uncomment for Colab\n",
        "!pip install \"ray[rllib]\" torch\n",
        "!pip install nashpy\n",
        "!pip install pickle5\n",
        "!git clone https://github.com/SammyDMartin/CC\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "Hj60SH87SPT0",
        "outputId": "78a33d20-fb5c-442c-eafb-57d448d8f896"
      },
      "outputs": [],
      "source": [
        "from cc_search import assess_agent_vs_others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1YOuBZPwSk_F"
      },
      "outputs": [],
      "source": [
        "from cc_tools import DQNTorchPolicy,DQNTrainer,PPOTorchPolicy,PPOTrainer,A3CTorchPolicy,A3CTrainer\n",
        "from cc_tools import maximin_policy,ideal_selfplay_policy,random_agent\n",
        "from cc_tools import TFTAverage,RandomPolicy,IteratedPrisonersDilemma\n",
        "\n",
        "from cc_search import get_rllib_config_eval\n",
        "import numpy as np\n",
        "from ray import tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tJMhFKwTbZuh"
      },
      "outputs": [],
      "source": [
        "class LogPolicy(RandomPolicy):\n",
        "    \"\"\"Play the move that would beat the last move of the opponent.\"\"\"\n",
        "    def __init__(self, observation_space, action_space, config):\n",
        "        super().__init__(observation_space, action_space, config)\n",
        "        self.obs_memory = []\n",
        "        self.act_memory = []\n",
        "        self.num = 1\n",
        "\n",
        "    def update_target(self):\n",
        "        pass\n",
        "    def get_weights(self):\n",
        "        pass\n",
        "    def compute_actions(self,\n",
        "                        obs_batch,\n",
        "                        state_batches=None,\n",
        "                        prev_action_batch=None,\n",
        "                        prev_reward_batch=None,\n",
        "                        **kwargs):\n",
        "        input = obs_batch[0]\n",
        "        self.obs_memory.append(float(prev_reward_batch))\n",
        "        self.act_memory.append(float(prev_action_batch))\n",
        "\n",
        "        output = np.random.choice([0,1])\n",
        "        n = 12\n",
        "\n",
        "        #print(\"Agent{}\".format(self.num),state_batches,prev_action_batch,prev_reward_batch)\n",
        "        #print(\"Agent{}\".format(self.num),input,output,flush=True)\n",
        "\n",
        "        if len(self.obs_memory)>n+1:\n",
        "            raise InterruptedError\n",
        "\n",
        "        if len(self.obs_memory)>n:\n",
        "            print(\"Agent{} obs\".format(self.num),self.obs_memory)\n",
        "            print(\"Agent{} acts\".format(self.num),self.act_memory)\n",
        "\n",
        "\n",
        "        return np.array([output]), \\\n",
        "                [], {}\n",
        "\n",
        "class LogPolicy2(LogPolicy):\n",
        "    \"\"\"Play the move that would beat the last move of the opponent.\"\"\"\n",
        "    def __init__(self, observation_space, action_space, config):\n",
        "        super().__init__(observation_space, action_space, config)\n",
        "        self.num = 2\n",
        "\n",
        "\n",
        "test_pol = lambda x,y: LogPolicy\n",
        "\n",
        "strategies = [test_pol]\n",
        "opponents = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tUqfhC0CS4zF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=13384)\u001b[0m 2022-01-24 14:24:26,807\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=13384)\u001b[0m 2022-01-24 14:24:26,825\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=13384)\u001b[0m 2022-01-24 14:24:26,834\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=13384)\u001b[0m Agent1 obs [0.0, -2.0, -1.0, 0.0, -2.0, 0.0, 0.0, -3.0, 0.0, 0.0, -1.0, -3.0, 0.0]\n",
            " pid=13384)\u001b[0m Agent1 acts [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
            " pid=13384)\u001b[0m Agent2 obs [0.0, -2.0, -1.0, 0.0, -2.0, -3.0, 0.0, 0.0, -3.0, 0.0, -1.0, 0.0, 0.0]\n",
            " pid=13384)\u001b[0m Agent2 acts [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-24 14:24:27,435\tERROR trial_runner.py:958 -- Trial DQN_IteratedPrisonersDilemma_4f81d_00000: Error processing event.\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\tune\\trial_runner.py\", line 924, in _process_trial\n",
            "    results = self.trial_executor.fetch_result(trial)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\tune\\ray_trial_executor.py\", line 787, in fetch_result\n",
            "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 105, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\worker.py\", line 1713, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(InterruptedError): \u001b[36mray::DQN.train()\u001b[39m (pid=13384, ip=127.0.0.1, repr=DQN)\n",
            "  File \"python\\ray\\_raylet.pyx\", line 625, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 629, in ray._raylet.execute_task\n",
            "  File \"python\\ray\\_raylet.pyx\", line 578, in ray._raylet.execute_task.function_executor\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 609, in actor_method_executor\n",
            "    return method(__ray_actor, *args, **kwargs)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 451, in _resume_span\n",
            "    return method(self, *_args, **_kwargs)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\tune\\trainable.py\", line 314, in train\n",
            "    result = self.step()\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 451, in _resume_span\n",
            "    return method(self, *_args, **_kwargs)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 885, in step\n",
            "    raise e\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 867, in step\n",
            "    result = self.step_attempt()\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 451, in _resume_span\n",
            "    return method(self, *_args, **_kwargs)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\agents\\trainer.py\", line 920, in step_attempt\n",
            "    step_results = next(self.train_exec_impl)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 756, in __next__\n",
            "    return next(self.built_iterator)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 843, in apply_filter\n",
            "    for item in it:\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 843, in apply_filter\n",
            "    for item in it:\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 843, in apply_filter\n",
            "    for item in it:\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 1075, in build_union\n",
            "    item = next(it)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 756, in __next__\n",
            "    return next(self.built_iterator)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\util\\iter.py\", line 783, in apply_foreach\n",
            "    for item in it:\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\execution\\rollout_ops.py\", line 76, in sampler\n",
            "    yield workers.local_worker().sample()\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 757, in sample\n",
            "    batches = [self.input_reader.next()]\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 103, in next\n",
            "    batches = [self.get_data()]\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 265, in get_data\n",
            "    item = next(self._env_runner)\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 660, in _env_runner\n",
            "    active_episodes=active_episodes,\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\evaluation\\sampler.py\", line 1073, in _do_policy_eval\n",
            "    episodes=[active_episodes[t.env_id] for t in eval_data])\n",
            "  File \"C:\\Users\\User\\anaconda3\\lib\\site-packages\\ray\\rllib\\policy\\policy.py\", line 293, in compute_actions_from_input_dict\n",
            "    **kwargs,\n",
            "  File \"<ipython-input-31-86f913d3c556>\", line 30, in compute_actions\n",
            "InterruptedError\n"
          ]
        },
        {
          "ename": "TuneError",
          "evalue": "('Trials did not complete', [DQN_IteratedPrisonersDilemma_4f81d_00000])",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-32-6fae72cbde8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrllib_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mcheckpoint_at_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m )\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ray\\tune\\tune.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, queue_trials, loggers, _remote)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trials did not complete\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trials did not complete: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTuneError\u001b[0m: ('Trials did not complete', [DQN_IteratedPrisonersDilemma_4f81d_00000])"
          ]
        }
      ],
      "source": [
        "#res1, res_full = assess_agent_vs_others(agent_trainer=(DQNTorchPolicy,DQNTrainer),opponents=opponents, strategies=strategies,steps=2)\n",
        "\n",
        "rllib_config, stop_config = get_rllib_config_eval(\n",
        "    seeds=[1],\n",
        "    steps=3,\n",
        "    game=IteratedPrisonersDilemma,\n",
        "    debug=False,\n",
        "    stop_iters=1,\n",
        "    policy_classes=[LogPolicy,LogPolicy2],\n",
        "    checkpoint_paths=[]\n",
        ")\n",
        "\n",
        "tune_analysis = tune.run(\n",
        "    # Works with any Trainer but then using PGTrainer is more safe (simple training loop)... but PGTrainer doesn't work here for some reasons...\n",
        "    DQNTrainer,\n",
        "    # PPOTrainer,\n",
        "    config=rllib_config,\n",
        "    stop=stop_config,\n",
        "    checkpoint_at_end=True, verbose = 0\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "obs_space.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
