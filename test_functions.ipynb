{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SammyDMartin/CC/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hj60SH87SPT0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ray version: 1.9.2\n"
          ]
        }
      ],
      "source": [
        "#Uncomment for Colab\n",
        "use_cc = False\n",
        "\"\"\"\n",
        "!pip install \"ray[rllib]\" torch\n",
        "!pip install nashpy\n",
        "!pip install pickle5\n",
        "!git clone https://github.com/SammyDMartin/CC.git\n",
        "!cd CC\n",
        "\n",
        "use_CC = True\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "if use_cc == True:\n",
        "    from CC.cc_search import assess_agent_vs_others,assess_agent_vs_agent,make_selfplay_checkpoint\n",
        "    from CC.cc_tools import DQNTorchPolicy,DQNTrainer,PPOTorchPolicy,PPOTrainer,A3CTorchPolicy,A3CTrainer,CoopPolicy,DefectPolicy,RandPolicy,UncoopExploiterPolicy, BullyPolicy, MaximinPolicy,MaximaxPolicy,MindlessCoopPolicy,MindlessCoopSecurityPolicy\n",
        "    from CC.cc_tools import IteratedPrisonersDilemma, IteratedChicken,IteratedStagHunt, IteratedBoS, MutalismCoordination, PureCoordination,IteratedAsymBoS\n",
        "else:\n",
        "    from cc_search import *\n",
        "    from cc_tools import DQNTorchPolicy,DQNTrainer,PPOTorchPolicy,PPOTrainer,CoopPolicy,DefectPolicy,RandPolicy,UncoopExploiterPolicy, BullyPolicy, MaximinPolicy,MaximaxPolicy,MindlessCoopPolicy,MindlessCoopSecurityPolicy\n",
        "    from cc_tools import PreferActOne,PreferActZero,GrimTrigger,GrimTriggerTwo\n",
        "    from cc_tools import IteratedPrisonersDilemma, IteratedChicken,IteratedStagHunt, IteratedBoS, MutalismCoordination, PureCoordination,IteratedAsymBoS\n",
        "    from cc_tools import bully, security_vals,maximin,nash_from_RLLIB,maximax,mindless_cooperate\n",
        "    from cc_tools import IteratedBoS, IteratedPrisonersDilemma,test_matrix,pretty_print_matrix\n",
        "    from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ngames = [IteratedPrisonersDilemma, IteratedChicken,IteratedStagHunt, IteratedBoS, MutalismCoordination, PureCoordination,IteratedAsymBoS]\\n   \\nfor g in games:\\n    test_matrix(g)\\n'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "games = [IteratedPrisonersDilemma, IteratedChicken,IteratedStagHunt, IteratedBoS, MutalismCoordination, PureCoordination,IteratedAsymBoS]\n",
        "   \n",
        "for g in games:\n",
        "    test_matrix(g)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\npretty_print_matrix(IteratedPrisonersDilemma)\\nprint(\"Row player cooperate, Column defect\")\\ncheck_output(CC_test_1)\\n\\npretty_print_matrix(IteratedPrisonersDilemma)\\nprint(\"Column player cooperate, Row defect\")\\ncheck_output(CC_test_2)\\n\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#CC_test_1,_ = rllib_train_ingame([CoopPolicy,DefectPolicy],IteratedPrisonersDilemma)\n",
        "#CC_test_2,_ = rllib_train_ingame([DefectPolicy,CoopPolicy],IteratedPrisonersDilemma)\n",
        "\n",
        "\"\"\"\n",
        "pretty_print_matrix(IteratedPrisonersDilemma)\n",
        "print(\"Row player cooperate, Column defect\")\n",
        "check_output(CC_test_1)\n",
        "\n",
        "pretty_print_matrix(IteratedPrisonersDilemma)\n",
        "print(\"Column player cooperate, Row defect\")\n",
        "check_output(CC_test_2)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tests single Uncooperative agent vs DQN\n",
        "\n",
        "## currently terrible self-play only trained and (unclear if works) gullible vs coop and uncoop trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-29 14:30:20,281\tERROR syncer.py:111 -- Log sync requires rsync to be installed.\n",
            " pid=13784)\u001b[0m 2022-04-29 14:30:25,075\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=13784)\u001b[0m 2022-04-29 14:30:25,075\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=13784)\u001b[0m 2022-04-29 14:30:25,205\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=13784)\u001b[0m 2022-04-29 14:30:25,235\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=13784)\u001b[0m 2022-04-29 14:30:25,257\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
            " pid=3688)\u001b[0m 2022-04-29 14:30:46,765\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=3688)\u001b[0m 2022-04-29 14:30:46,790\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=3688)\u001b[0m 2022-04-29 14:30:46,798\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=3688)\u001b[0m 2022-04-29 14:30:46,812\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=3688)\u001b[0m ray version: 1.9.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=9196)\u001b[0m 2022-04-29 14:31:03,344\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=9196)\u001b[0m 2022-04-29 14:31:03,344\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=9196)\u001b[0m 2022-04-29 14:31:03,363\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=9196)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=9196)\u001b[0m 2022-04-29 14:31:03,389\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=9196)\u001b[0m 2022-04-29 14:31:03,399\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=9196)\u001b[0m ray version: 1.9.2\n",
            " pid=9196)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-30-42\\DQN_IteratedBoS_91276_00000_0_seed=1_2022-04-29_14-30-42\\checkpoint_000010\\checkpoint-10\n",
            " pid=1292)\u001b[0m ray version: 1.9.2\n",
            " pid=1292)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-30-59\\DQN_IteratedBoS_9b050_00000_0_seed=1_2022-04-29_14-30-59\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=1292)\u001b[0m 2022-04-29 14:31:21,958\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=1292)\u001b[0m 2022-04-29 14:31:21,958\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=1292)\u001b[0m 2022-04-29 14:31:21,977\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=1292)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=1292)\u001b[0m 2022-04-29 14:31:22,004\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=1292)\u001b[0m 2022-04-29 14:31:22,014\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
            " pid=2456)\u001b[0m 2022-04-29 14:31:47,412\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=2456)\u001b[0m 2022-04-29 14:31:47,412\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=2456)\u001b[0m 2022-04-29 14:31:47,433\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=2456)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=2456)\u001b[0m 2022-04-29 14:31:47,467\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=2456)\u001b[0m 2022-04-29 14:31:47,480\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=2456)\u001b[0m ray version: 1.9.2\n",
            " pid=2456)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-31-17\\DQN_IteratedBoS_a62a8_00000_0_seed=1_2022-04-29_14-31-17\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=1008)\u001b[0m 2022-04-29 14:32:06,056\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=1008)\u001b[0m 2022-04-29 14:32:06,057\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=1008)\u001b[0m 2022-04-29 14:32:06,080\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=1008)\u001b[0m 2022-04-29 14:32:06,105\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=1008)\u001b[0m 2022-04-29 14:32:06,115\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
            " pid=7876)\u001b[0m 2022-04-29 14:32:27,996\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=7876)\u001b[0m 2022-04-29 14:32:27,996\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=7876)\u001b[0m 2022-04-29 14:32:28,017\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=7876)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=7876)\u001b[0m 2022-04-29 14:32:28,058\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=7876)\u001b[0m 2022-04-29 14:32:28,071\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=7876)\u001b[0m ray version: 1.9.2\n",
            " pid=7876)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-32-01\\DQN_IteratedBoS_c0143_00000_0_seed=1_2022-04-29_14-32-01\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=4256)\u001b[0m 2022-04-29 14:32:48,704\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=4256)\u001b[0m 2022-04-29 14:32:48,704\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=4256)\u001b[0m 2022-04-29 14:32:48,725\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=4256)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=4256)\u001b[0m 2022-04-29 14:32:48,759\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=4256)\u001b[0m 2022-04-29 14:32:48,777\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=4256)\u001b[0m ray version: 1.9.2\n",
            " pid=4256)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-32-17\\DQN_IteratedBoS_c9e20_00000_0_seed=1_2022-04-29_14-32-17\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=6132)\u001b[0m 2022-04-29 14:33:14,458\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=6132)\u001b[0m 2022-04-29 14:33:14,458\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=6132)\u001b[0m 2022-04-29 14:33:14,479\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=6132)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=6132)\u001b[0m 2022-04-29 14:33:14,514\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=6132)\u001b[0m 2022-04-29 14:33:14,531\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=6132)\u001b[0m ray version: 1.9.2\n",
            " pid=6132)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-32-43\\DQN_IteratedBoS_d9814_00000_0_seed=1_2022-04-29_14-32-43\\checkpoint_000010\\checkpoint-10\n",
            " pid=9188)\u001b[0m ray version: 1.9.2\n",
            " pid=9188)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-33-04\\DQN_IteratedBoS_e5d4d_00000_0_seed=1_2022-04-29_14-33-04\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=9188)\u001b[0m 2022-04-29 14:33:34,147\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=9188)\u001b[0m 2022-04-29 14:33:34,147\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=9188)\u001b[0m 2022-04-29 14:33:34,166\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=9188)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=9188)\u001b[0m 2022-04-29 14:33:34,198\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=9188)\u001b[0m 2022-04-29 14:33:34,208\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
            " pid=17380)\u001b[0m 2022-04-29 14:33:59,401\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=17380)\u001b[0m 2022-04-29 14:33:59,401\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=17380)\u001b[0m 2022-04-29 14:33:59,421\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=17380)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=17380)\u001b[0m 2022-04-29 14:33:59,465\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=17380)\u001b[0m 2022-04-29 14:33:59,478\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=17380)\u001b[0m ray version: 1.9.2\n",
            " pid=17380)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-33-29\\DQN_IteratedBoS_f4c0b_00000_0_seed=1_2022-04-29_14-33-29\\checkpoint_000010\\checkpoint-10\n",
            " pid=1596)\u001b[0m ray version: 1.9.2\n",
            " pid=1596)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-33-49\\DQN_IteratedBoS_007fd_00000_0_seed=1_2022-04-29_14-33-49\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=1596)\u001b[0m 2022-04-29 14:34:19,165\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=1596)\u001b[0m 2022-04-29 14:34:19,165\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=1596)\u001b[0m 2022-04-29 14:34:19,183\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=1596)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=1596)\u001b[0m 2022-04-29 14:34:19,218\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=1596)\u001b[0m 2022-04-29 14:34:19,232\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=11844)\u001b[0m ray version: 1.9.2\n",
            " pid=11844)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-34-14\\DQN_IteratedBoS_0f842_00000_0_seed=1_2022-04-29_14-34-14\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=11844)\u001b[0m 2022-04-29 14:34:46,498\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=11844)\u001b[0m 2022-04-29 14:34:46,498\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=11844)\u001b[0m 2022-04-29 14:34:46,520\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=11844)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=11844)\u001b[0m 2022-04-29 14:34:46,556\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=11844)\u001b[0m 2022-04-29 14:34:46,569\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n",
            " pid=14436)\u001b[0m 2022-04-29 14:35:06,187\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=14436)\u001b[0m 2022-04-29 14:35:06,187\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=14436)\u001b[0m 2022-04-29 14:35:06,210\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=14436)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=14436)\u001b[0m 2022-04-29 14:35:06,243\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=14436)\u001b[0m 2022-04-29 14:35:06,256\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=14436)\u001b[0m ray version: 1.9.2\n",
            " pid=14436)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-34-35\\DQN_IteratedBoS_1c290_00000_0_seed=1_2022-04-29_14-34-35\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=7440)\u001b[0m 2022-04-29 14:35:31,243\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=7440)\u001b[0m 2022-04-29 14:35:31,243\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=7440)\u001b[0m 2022-04-29 14:35:31,265\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=7440)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=7440)\u001b[0m 2022-04-29 14:35:31,303\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=7440)\u001b[0m 2022-04-29 14:35:31,315\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=7440)\u001b[0m ray version: 1.9.2\n",
            " pid=7440)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-35-01\\DQN_IteratedBoS_2ba09_00000_0_seed=1_2022-04-29_14-35-01\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=6084)\u001b[0m 2022-04-29 14:35:50,980\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=6084)\u001b[0m 2022-04-29 14:35:50,980\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=6084)\u001b[0m 2022-04-29 14:35:51,000\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=6084)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=6084)\u001b[0m 2022-04-29 14:35:51,033\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=6084)\u001b[0m 2022-04-29 14:35:51,045\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=6084)\u001b[0m ray version: 1.9.2\n",
            " pid=6084)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-35-21\\DQN_IteratedBoS_37575_00000_0_seed=1_2022-04-29_14-35-21\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=16724)\u001b[0m 2022-04-29 14:36:16,204\tINFO dqn.py:142 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=16724)\u001b[0m 2022-04-29 14:36:16,204\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=16724)\u001b[0m 2022-04-29 14:36:16,238\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=16724)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=16724)\u001b[0m 2022-04-29 14:36:16,293\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=16724)\u001b[0m 2022-04-29 14:36:16,306\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=16724)\u001b[0m ray version: 1.9.2\n",
            " pid=16724)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-35-46\\DQN_IteratedBoS_46549_00000_0_seed=1_2022-04-29_14-35-46\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nprint(\"Self-play trained\")\\nprint(check_output(results_self))\\nprint(\"Trained vs pure strats\")\\nprint(check_output(results_gull))\\nprint(\"Trained vs general strats\")\\nprint(check_output(results_gen))\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#cp,output_1 = make_botplay_checkpoint(IteratedPrisonersDilemma,(PPOTorchPolicy,PPOTrainer),CoopPolicy,10)\n",
        "\n",
        "game_test = IteratedBoS\n",
        "\n",
        "#ag_train = (PPOTorchPolicy,PPOTrainer)\n",
        "\n",
        "ag_train = (DQNTorchPolicy,DQNTrainer)\n",
        "\n",
        "checkpoint_self,results_self = make_selfplay_checkpoint(game_test,ag_train,steps = 10)\n",
        "\n",
        "unrol_df(results_self)\n",
        " \n",
        "checkpoint_gull,results_gull = make_gullible_checkpoint(game_test,ag_train,train_repeats=2,steps=5)\n",
        "\n",
        "checkpoint_general,results_gen = make_general_checkpoint(game_test,ag_train,train_repeats=2,steps=5)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print(\"Self-play trained\")\n",
        "print(check_output(results_self))\n",
        "print(\"Trained vs pure strats\")\n",
        "print(check_output(results_gull))\n",
        "print(\"Trained vs general strats\")\n",
        "print(check_output(results_gen))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=16748)\u001b[0m ray version: 1.9.2\n",
            " pid=16748)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\test\\DQN_IteratedBoS_83b97_00000_0_2022-04-29_14-30-20\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=16748)\u001b[0m 2022-04-29 14:36:41,482\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=16748)\u001b[0m 2022-04-29 14:36:41,482\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=16748)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=16748)\u001b[0m 2022-04-29 14:36:41,570\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=16748)\u001b[0m 2022-04-29 14:36:41,579\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=16748)\u001b[0m 2022-04-29 14:36:41,604\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=6396)\u001b[0m ray version: 1.9.2\n",
            " pid=6396)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\test\\DQN_IteratedBoS_83b97_00000_0_2022-04-29_14-30-20\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=6396)\u001b[0m 2022-04-29 14:37:50,812\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=6396)\u001b[0m 2022-04-29 14:37:50,812\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=6396)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=6396)\u001b[0m 2022-04-29 14:37:50,845\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=6396)\u001b[0m 2022-04-29 14:37:50,850\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=6396)\u001b[0m 2022-04-29 14:37:50,861\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self-play only results vs coop:\n",
            "(0    1.5\n",
            "1    1.5\n",
            "2    1.5\n",
            "3    1.5\n",
            "4    1.5\n",
            "Name: policy_reward_mean/player_row, dtype: float64, 0    1.0\n",
            "1    1.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "Name: policy_reward_mean/player_col, dtype: float64)\n",
            "self-play only results vs defect:\n",
            "(0    1.5\n",
            "1    1.5\n",
            "2    1.5\n",
            "3    1.5\n",
            "4    1.5\n",
            "Name: policy_reward_mean/player_row, dtype: float64, 0    1.0\n",
            "1    1.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "Name: policy_reward_mean/player_col, dtype: float64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=6068)\u001b[0m 2022-04-29 14:38:49,102\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=6068)\u001b[0m 2022-04-29 14:38:49,103\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=6068)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=6068)\u001b[0m 2022-04-29 14:38:49,148\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=6068)\u001b[0m 2022-04-29 14:38:49,152\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=6068)\u001b[0m 2022-04-29 14:38:49,165\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=6068)\u001b[0m ray version: 1.9.2\n",
            " pid=6068)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-31-35\\DQN_IteratedBoS_b0d1b_00000_0_seed=1_2022-04-29_14-31-35\\checkpoint_000010\\checkpoint-10\n",
            " pid=9744)\u001b[0m ray version: 1.9.2\n",
            " pid=9744)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-31-35\\DQN_IteratedBoS_b0d1b_00000_0_seed=1_2022-04-29_14-31-35\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=9744)\u001b[0m 2022-04-29 14:39:51,768\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=9744)\u001b[0m 2022-04-29 14:39:51,768\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=9744)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=9744)\u001b[0m 2022-04-29 14:39:51,799\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=9744)\u001b[0m 2022-04-29 14:39:51,807\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=9744)\u001b[0m 2022-04-29 14:39:51,819\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trained vs cooperators and defectors results vs coop:\n",
            "(0    1.5\n",
            "1    1.5\n",
            "2    1.5\n",
            "3    1.5\n",
            "4    1.5\n",
            "Name: policy_reward_mean/player_row, dtype: float64, 0    1.0\n",
            "1    1.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "Name: policy_reward_mean/player_col, dtype: float64)\n",
            "trained vs cooperators and defectors results vs defect:\n",
            "(0    1.5\n",
            "1    1.5\n",
            "2    1.5\n",
            "3    1.5\n",
            "4    1.5\n",
            "Name: policy_reward_mean/player_row, dtype: float64, 0    1.0\n",
            "1    1.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "Name: policy_reward_mean/player_col, dtype: float64)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=13652)\u001b[0m 2022-04-29 14:40:49,939\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=13652)\u001b[0m 2022-04-29 14:40:49,940\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=13652)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=13652)\u001b[0m 2022-04-29 14:40:49,981\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=13652)\u001b[0m 2022-04-29 14:40:49,986\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=13652)\u001b[0m 2022-04-29 14:40:49,997\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=13652)\u001b[0m ray version: 1.9.2\n",
            " pid=13652)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-36-05\\DQN_IteratedBoS_51aa5_00000_0_seed=1_2022-04-29_14-36-05\\checkpoint_000010\\checkpoint-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " pid=9192)\u001b[0m 2022-04-29 14:41:54,032\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            " pid=9192)\u001b[0m 2022-04-29 14:41:54,032\tINFO trainer.py:745 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            " pid=9192)\u001b[0m restoring ckpt: not loading objs['filters']\n",
            " pid=9192)\u001b[0m 2022-04-29 14:41:54,068\tWARNING deprecation.py:46 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
            " pid=9192)\u001b[0m 2022-04-29 14:41:54,072\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n",
            " pid=9192)\u001b[0m 2022-04-29 14:41:54,084\tWARNING deprecation.py:46 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " pid=9192)\u001b[0m ray version: 1.9.2\n",
            " pid=9192)\u001b[0m going to load policy player_row from checkpoint C:\\Users\\User\\ray_results\\DQN_2022-04-29_14-36-05\\DQN_IteratedBoS_51aa5_00000_0_seed=1_2022-04-29_14-36-05\\checkpoint_000010\\checkpoint-10\n",
            "trained vs all of it results vs coop:\n",
            "(0    1.5\n",
            "1    1.5\n",
            "2    1.5\n",
            "3    1.5\n",
            "4    1.5\n",
            "Name: policy_reward_mean/player_row, dtype: float64, 0    1.0\n",
            "1    1.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "Name: policy_reward_mean/player_col, dtype: float64)\n",
            "trained vs all of it results vs defect:\n",
            "(0    1.5\n",
            "1    1.5\n",
            "2    1.5\n",
            "3    1.5\n",
            "4    1.5\n",
            "Name: policy_reward_mean/player_row, dtype: float64, 0    1.0\n",
            "1    1.0\n",
            "2    1.0\n",
            "3    1.0\n",
            "4    1.0\n",
            "Name: policy_reward_mean/player_col, dtype: float64)\n"
          ]
        }
      ],
      "source": [
        "names = [\"self-play only\",\"trained vs cooperators and defectors\",\"trained vs all of it\"]\n",
        "\n",
        "checkpoints = [checkpoint_self,checkpoint_gull,checkpoint_general]\n",
        "\n",
        "#checkpoints = [checkpoint_self]\n",
        "\n",
        "for idx,checkpoint in enumerate(checkpoints):\n",
        "    results_vs_coop = assess_agent_vs_agent(policies=[ag_train[0],PreferActZero],checkpoints=[checkpoint],environment=IteratedBoS,iters=5,trainer=PPOTrainer)\n",
        "    results_vs_defect = assess_agent_vs_agent(policies=[ag_train[0],PreferActOne],checkpoints=[checkpoint],environment=IteratedBoS,iters=5,trainer=PPOTrainer)\n",
        "    \n",
        "    print(\"{} results vs coop:\".format(names[idx]))\n",
        "    print(check_output(results_vs_coop))\n",
        "    print(\"{} results vs defect:\".format(names[idx]))\n",
        "    print(check_output(results_vs_defect))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_reward_max</th>\n",
              "      <th>episode_reward_min</th>\n",
              "      <th>episode_reward_mean</th>\n",
              "      <th>episode_len_mean</th>\n",
              "      <th>episodes_this_iter</th>\n",
              "      <th>num_healthy_workers</th>\n",
              "      <th>timesteps_total</th>\n",
              "      <th>timesteps_this_iter</th>\n",
              "      <th>agent_timesteps_total</th>\n",
              "      <th>done</th>\n",
              "      <th>...</th>\n",
              "      <th>timers/load_time_ms</th>\n",
              "      <th>timers/load_throughput</th>\n",
              "      <th>timers/learn_time_ms</th>\n",
              "      <th>timers/learn_throughput</th>\n",
              "      <th>info/num_steps_sampled</th>\n",
              "      <th>info/num_agent_steps_sampled</th>\n",
              "      <th>info/num_steps_trained</th>\n",
              "      <th>info/num_agent_steps_trained</th>\n",
              "      <th>perf/cpu_util_percent</th>\n",
              "      <th>perf/ram_util_percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>4000</td>\n",
              "      <td>0</td>\n",
              "      <td>8000</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4000</td>\n",
              "      <td>8000</td>\n",
              "      <td>4000</td>\n",
              "      <td>0</td>\n",
              "      <td>27.940000</td>\n",
              "      <td>88.433333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>8000</td>\n",
              "      <td>0</td>\n",
              "      <td>16000</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8000</td>\n",
              "      <td>16000</td>\n",
              "      <td>8000</td>\n",
              "      <td>0</td>\n",
              "      <td>29.126667</td>\n",
              "      <td>88.593333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>12000</td>\n",
              "      <td>0</td>\n",
              "      <td>24000</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12000</td>\n",
              "      <td>24000</td>\n",
              "      <td>12000</td>\n",
              "      <td>0</td>\n",
              "      <td>29.093333</td>\n",
              "      <td>87.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>16000</td>\n",
              "      <td>0</td>\n",
              "      <td>32000</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16000</td>\n",
              "      <td>32000</td>\n",
              "      <td>16000</td>\n",
              "      <td>0</td>\n",
              "      <td>38.041176</td>\n",
              "      <td>87.258824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>20000</td>\n",
              "      <td>0</td>\n",
              "      <td>40000</td>\n",
              "      <td>True</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20000</td>\n",
              "      <td>40000</td>\n",
              "      <td>20000</td>\n",
              "      <td>0</td>\n",
              "      <td>33.100000</td>\n",
              "      <td>87.320000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
              "0                25.0                25.0                 25.0   \n",
              "1                25.0                25.0                 25.0   \n",
              "2                25.0                25.0                 25.0   \n",
              "3                25.0                25.0                 25.0   \n",
              "4                25.0                25.0                 25.0   \n",
              "\n",
              "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
              "0              10.0                 400                    0             4000   \n",
              "1              10.0                 400                    0             8000   \n",
              "2              10.0                 400                    0            12000   \n",
              "3              10.0                 400                    0            16000   \n",
              "4              10.0                 400                    0            20000   \n",
              "\n",
              "   timesteps_this_iter  agent_timesteps_total   done  ...  \\\n",
              "0                    0                   8000  False  ...   \n",
              "1                    0                  16000  False  ...   \n",
              "2                    0                  24000  False  ...   \n",
              "3                    0                  32000  False  ...   \n",
              "4                    0                  40000   True  ...   \n",
              "\n",
              "   timers/load_time_ms  timers/load_throughput timers/learn_time_ms  \\\n",
              "0                  0.0                     0.0                  0.0   \n",
              "1                  0.0                     0.0                  0.0   \n",
              "2                  0.0                     0.0                  0.0   \n",
              "3                  0.0                     0.0                  0.0   \n",
              "4                  0.0                     0.0                  0.0   \n",
              "\n",
              "  timers/learn_throughput info/num_steps_sampled  \\\n",
              "0                     0.0                   4000   \n",
              "1                     0.0                   8000   \n",
              "2                     0.0                  12000   \n",
              "3                     0.0                  16000   \n",
              "4                     0.0                  20000   \n",
              "\n",
              "   info/num_agent_steps_sampled  info/num_steps_trained  \\\n",
              "0                          8000                    4000   \n",
              "1                         16000                    8000   \n",
              "2                         24000                   12000   \n",
              "3                         32000                   16000   \n",
              "4                         40000                   20000   \n",
              "\n",
              "   info/num_agent_steps_trained  perf/cpu_util_percent perf/ram_util_percent  \n",
              "0                             0              27.940000             88.433333  \n",
              "1                             0              29.126667             88.593333  \n",
              "2                             0              29.093333             87.033333  \n",
              "3                             0              38.041176             87.258824  \n",
              "4                             0              33.100000             87.320000  \n",
              "\n",
              "[5 rows x 51 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unrol_df(results_vs_coop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_reward_max</th>\n",
              "      <th>episode_reward_min</th>\n",
              "      <th>episode_reward_mean</th>\n",
              "      <th>episode_len_mean</th>\n",
              "      <th>episodes_this_iter</th>\n",
              "      <th>num_healthy_workers</th>\n",
              "      <th>timesteps_total</th>\n",
              "      <th>timesteps_this_iter</th>\n",
              "      <th>agent_timesteps_total</th>\n",
              "      <th>done</th>\n",
              "      <th>...</th>\n",
              "      <th>timers/load_time_ms</th>\n",
              "      <th>timers/load_throughput</th>\n",
              "      <th>timers/learn_time_ms</th>\n",
              "      <th>timers/learn_throughput</th>\n",
              "      <th>info/num_steps_sampled</th>\n",
              "      <th>info/num_agent_steps_sampled</th>\n",
              "      <th>info/num_steps_trained</th>\n",
              "      <th>info/num_agent_steps_trained</th>\n",
              "      <th>perf/cpu_util_percent</th>\n",
              "      <th>perf/ram_util_percent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>4000</td>\n",
              "      <td>0</td>\n",
              "      <td>8000</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4000</td>\n",
              "      <td>8000</td>\n",
              "      <td>4000</td>\n",
              "      <td>0</td>\n",
              "      <td>28.413333</td>\n",
              "      <td>81.686667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>8000</td>\n",
              "      <td>0</td>\n",
              "      <td>16000</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8000</td>\n",
              "      <td>16000</td>\n",
              "      <td>8000</td>\n",
              "      <td>0</td>\n",
              "      <td>29.353333</td>\n",
              "      <td>81.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>12000</td>\n",
              "      <td>0</td>\n",
              "      <td>24000</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12000</td>\n",
              "      <td>24000</td>\n",
              "      <td>12000</td>\n",
              "      <td>0</td>\n",
              "      <td>28.333333</td>\n",
              "      <td>81.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>16000</td>\n",
              "      <td>0</td>\n",
              "      <td>32000</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16000</td>\n",
              "      <td>32000</td>\n",
              "      <td>16000</td>\n",
              "      <td>0</td>\n",
              "      <td>31.073333</td>\n",
              "      <td>80.826667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>400</td>\n",
              "      <td>0</td>\n",
              "      <td>20000</td>\n",
              "      <td>0</td>\n",
              "      <td>40000</td>\n",
              "      <td>True</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20000</td>\n",
              "      <td>40000</td>\n",
              "      <td>20000</td>\n",
              "      <td>0</td>\n",
              "      <td>28.106667</td>\n",
              "      <td>80.493333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
              "0                25.0                25.0                 25.0   \n",
              "1                25.0                25.0                 25.0   \n",
              "2                25.0                25.0                 25.0   \n",
              "3                25.0                25.0                 25.0   \n",
              "4                25.0                25.0                 25.0   \n",
              "\n",
              "   episode_len_mean  episodes_this_iter  num_healthy_workers  timesteps_total  \\\n",
              "0              10.0                 400                    0             4000   \n",
              "1              10.0                 400                    0             8000   \n",
              "2              10.0                 400                    0            12000   \n",
              "3              10.0                 400                    0            16000   \n",
              "4              10.0                 400                    0            20000   \n",
              "\n",
              "   timesteps_this_iter  agent_timesteps_total   done  ...  \\\n",
              "0                    0                   8000  False  ...   \n",
              "1                    0                  16000  False  ...   \n",
              "2                    0                  24000  False  ...   \n",
              "3                    0                  32000  False  ...   \n",
              "4                    0                  40000   True  ...   \n",
              "\n",
              "   timers/load_time_ms  timers/load_throughput timers/learn_time_ms  \\\n",
              "0                  0.0                     0.0                  0.0   \n",
              "1                  0.0                     0.0                  0.0   \n",
              "2                  0.0                     0.0                  0.0   \n",
              "3                  0.0                     0.0                  0.0   \n",
              "4                  0.0                     0.0                  0.0   \n",
              "\n",
              "  timers/learn_throughput info/num_steps_sampled  \\\n",
              "0                     0.0                   4000   \n",
              "1                     0.0                   8000   \n",
              "2                     0.0                  12000   \n",
              "3                     0.0                  16000   \n",
              "4                     0.0                  20000   \n",
              "\n",
              "   info/num_agent_steps_sampled  info/num_steps_trained  \\\n",
              "0                          8000                    4000   \n",
              "1                         16000                    8000   \n",
              "2                         24000                   12000   \n",
              "3                         32000                   16000   \n",
              "4                         40000                   20000   \n",
              "\n",
              "   info/num_agent_steps_trained  perf/cpu_util_percent perf/ram_util_percent  \n",
              "0                             0              28.413333             81.686667  \n",
              "1                             0              29.353333             81.740000  \n",
              "2                             0              28.333333             81.700000  \n",
              "3                             0              31.073333             80.826667  \n",
              "4                             0              28.106667             80.493333  \n",
              "\n",
              "[5 rows x 51 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unrol_df(results_vs_defect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Which agents to use in assesments?\n",
        "Agents that 95% of the time take either action 1 or 2, i.e. both pure strats\n",
        "Gullible agents - agents that assume opponents are playing stationary + pure strategies and try to exploit those. Will use uncooperative exploiter\n",
        "Grim Trigger with thresholds 1 and 2\n",
        "Regular pretrained bots\n",
        "Random agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "melting_pot_agents = [PreferActOne,PreferActZero,GrimTrigger,GrimTriggerTwo,UncoopExploiterPolicy,RandPolicy]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tests DQN vs Uncoop Agent on all games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'results_uncoop' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-9-1ca14759a690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#results_uncoop = assess_agent_vs_others((UncoopExploiterPolicy,None),opponents=opponents,strategies=strategies,steps=100)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mresults_uncoop\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'results_uncoop' is not defined"
          ]
        }
      ],
      "source": [
        "#strategies = [UncoopExploiterPolicy,MindlessCoopPolicy,MaximinPolicy,MaximaxPolicy,RandPolicy,BullyPolicy,MindlessCoopSecurityPolicy]\n",
        "#opponents = [(PPOTorchPolicy,PPOTrainer),(DQNTorchPolicy,DQNTrainer),(A3CTorchPolicy,A3CTrainer)]\n",
        "strategies = melting_pot_agents\n",
        "\n",
        "opponents = [(DQNTorchPolicy,DQNTrainer),(PPOTorchPolicy,PPOTrainer)]\n",
        "\n",
        "#results_uncoop = assess_agent_vs_others((UncoopExploiterPolicy,None),opponents=opponents,strategies=strategies,steps=100)\n",
        "results_uncoop[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_PPO = assess_agent_vs_others((PPOTorchPolicy,PPOTrainer),opponents=opponents,strategies=strategies,steps=20)\n",
        "results_PPO[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_uncoop[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_PPO[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalized results\n",
        "(note that the DQN did worse than the baseline on everything except for Chicken and Mutualism)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_PPO[0]-results_uncoop[0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVTPNg2bKbNUAxBs2gkd6L",
      "include_colab_link": true,
      "name": "test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
